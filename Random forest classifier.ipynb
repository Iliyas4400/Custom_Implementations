{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AjzGopb_YcKR"
   },
   "source": [
    "# Application of Bootstrap samples in Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZSCtDI6YcKT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2Y1Z1DoYcKZ"
   },
   "source": [
    " <li> Load the boston house dataset </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBWRNKCDYcKb"
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "x=boston.data #independent variables\n",
    "y=boston.target #target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTbK20-mWYHU",
    "outputId": "b473b251-a104-44df-f6c3-3427184c9042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.2489e-01, 1.2500e+01, 7.8700e+00, ..., 1.5200e+01, 3.9252e+02,\n",
       "        2.0450e+01],\n",
       "       [1.1747e-01, 1.2500e+01, 7.8700e+00, ..., 1.5200e+01, 3.9690e+02,\n",
       "        1.3270e+01],\n",
       "       [9.3780e-02, 1.2500e+01, 7.8700e+00, ..., 1.5200e+01, 3.9050e+02,\n",
       "        1.5710e+01],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.shape)\n",
    "x[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store all the target values in a dictionary along with row index as key\n",
    "D={i:y[i] for i in range(0,len(y))} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJ_FwP7xYcKg"
   },
   "source": [
    "### Task: 1\n",
    "<font color='red'><b>Step 1 Creating samples: </b></font> Randomly create 30 samples from the whole boston data points.\n",
    "<ol>\n",
    "<li>Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points</li>\n",
    "<li>Ex: For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly consider we have selected [4, 5, 7, 8, 9, 3] now we will replciate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]</li>\n",
    "<li> we create 30 samples like this </li>\n",
    "<li> Note that as a part of the Bagging when you are taking the random samples make sure each of the sample will have                different set of columns</li>\n",
    "<li> Ex: assume we have 10 columns for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample [7, 9, 1, 4, 5, 6, 2] and so on...</li>\n",
    "<li> Make sure each sample will have atleast 3 feautres/columns/attributes</li>\n",
    "</ol>\n",
    "\n",
    "<font color='red'><b>Step 2 Building High Variance Models on each of the sample and finding train MSE value:</b></font> Build a DecisionTreeRegressor on each of the sample.\n",
    "<ol><li>Build a regression trees on each of 30 samples.</li>\n",
    "<li>computed the predicted values of each data point(506 data points) in your corpus.</li>\n",
    "<li> predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.</li>\n",
    "<li>Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$.</li>\n",
    "</ol>\n",
    "\n",
    "<font color='red'><b>Step 3 Calculating the OOB score :</b></font>\n",
    "<ol>\n",
    "<li>Computed the predicted values of each data point(506 data points) in your corpus.</li>\n",
    "<li>Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{k}\\sum_{\\text{k= model which was buit on samples not included } x^{i}}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.</li>\n",
    "<li>Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$.</li>\n",
    "</ol>\n",
    "\n",
    "### Task: 2\n",
    "<pre>\n",
    "<font color='red'><b>Computing CI of OOB Score and Train MSE</b></font>\n",
    "<ol>\n",
    "<li> Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score </li>\n",
    "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
    "<li> using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score </li>\n",
    "<li> you need to report CI of MSE and CI of OOB Score </li>\n",
    "<li> Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel</li>\n",
    "</ol>\n",
    "</pre>\n",
    "### Task: 3\n",
    "<pre>\n",
    "<font color='red'><b>Given a single query point predict the price of house.</b></font>\n",
    "\n",
    "<li>Consider xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] Predict the house price for this point as mentioned in the step 2 of Task 1. </li>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=boston.feature_names\n",
    "x=pd.DataFrame(data=boston.data, index=None, columns=columns, dtype=None, copy=False)\n",
    "y=pd.DataFrame(data=boston.target,columns=['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import random\n",
    "def random_select_rows(x_train): \n",
    "    c= random.sample(range(0, len(x_train)), int(0.6*len(x_train)))#randomly select 60 percent rows from X\n",
    "    oob=list(set(np.arange(0,506))-set(c))\n",
    "    k=np.random.choice(c, 203, replace=False)#sample 203 elements randomly from selected list\n",
    "    k=[i for i in k]#form a list of all the sampled indecies\n",
    "    return((c+k),oob)#add both the lists\n",
    "def random_select_columns(col):#randomly select colum indecies\n",
    "    return(np.random.choice(col,8,replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 30 Decision trees\n",
    "dtr1=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr2=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr3=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10) \n",
    "dtr4=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr5=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr6=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr7=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr8=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr9=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr10=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr11=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr12=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr13=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10) \n",
    "dtr14=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr15=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr16=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr17=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr18=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr19=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr20=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr21=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr22=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr23=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10) \n",
    "dtr24=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr25=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr26=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr27=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr28=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr29=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)\n",
    "dtr30=DecisionTreeRegressor(criterion='mse',random_state=0,min_samples_split=5,max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor={1:dtr1,2:dtr2,3:dtr3,4:dtr4,5:dtr5,6:dtr6,7:dtr7,8:dtr8,9:dtr9,10:dtr10 ,11:dtr11,12:dtr12,13:dtr13,14:dtr14,\n",
    "           15:dtr15,15:dtr15,16:dtr16,17:dtr17,18:dtr18,19:dtr19 ,20:dtr20,21:dtr21,22:dtr22,23:dtr23,\n",
    "           24:dtr24,25:dtr25,26:dtr26,27:dtr27,28:dtr28 ,29:dtr29,30:dtr30 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse():\n",
    "    #create 30samples and store therir row and column indecies\n",
    "    row_list=[]\n",
    "    col_list=[]\n",
    "    x_list=[]\n",
    "    y_list=[]\n",
    "    oob_list=[]\n",
    "    x_oob=[]\n",
    "    y_oob=[]\n",
    "    for i in range(0,30):\n",
    "        row,oob=random_select_rows(x)#row indecies\n",
    "        row_list.append(row)#list of row indecies\n",
    "        oob_list.append(oob)#list oob indecies\n",
    "        col=random_select_columns(13)#column indecies\n",
    "        col_list.append(col)#list of column indecies\n",
    "        x_list.append(x.iloc [row,col])\n",
    "        x_oob.append(x.iloc[oob,col])\n",
    "        y_list.append(y.iloc[row]) \n",
    "        y_oob.append(y.iloc[oob])\n",
    "    \n",
    "    #fit 30 trees with each sample of dataset\n",
    "    for i in range(1,31): \n",
    "        regressor[i].fit(x_list[i-1], y_list[i-1]) \n",
    "        \n",
    "    Y1=[]   #list of row indecies and predicted values of train data\n",
    "    Y2=[]   #list of row indecies and predicted values of OOB samples\n",
    "    for i in range(0,30): \n",
    "        y_pred=[] \n",
    "        y_oob_pred=[]\n",
    "        y_pred=regressor[i+1].predict(x_list[i])#predicted values of x_list\n",
    "        y_oob_pred=regressor[i+1].predict(x_oob[i])\n",
    "        l=row_list[i]\n",
    "        for j in range(506):\n",
    "            Y1.append([l[j],y_pred[j]]) #store predicted value along with its row index\n",
    "        l2=oob_list[i]\n",
    "        for k in range(203):\n",
    "            Y2.append([l2[k],y_oob_pred[k]]) #store predicted value along with its oob index\n",
    "    \n",
    "    #Dictionary to store predicted values with row indecies as key and output as values\n",
    "    D_train={i:[] for i in range(506)}\n",
    "    D_oob={i:[] for i in range(506)}\n",
    "    for i in Y1:\n",
    "        D_train[i[0]].append(i[1])\n",
    "    for i in Y2:\n",
    "        D_oob[i[0]].append(i[1])\n",
    "    \n",
    "    s1=0\n",
    "    for i in range(506):       #MSE\n",
    "        p=np.mean(D_train[i])\n",
    "        s1+=((D[i]-p)*(D[i]-p))\n",
    "     \n",
    "    s2=0\n",
    "    for i in range(506):      #OOB score\n",
    "        p=np.mean(D_oob[i])\n",
    "        s2+=((D[i]-p)*(D[i]-p))\n",
    "    \n",
    "    return(s1/506,s2/506,col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE :  0.2940923755854412\n",
      "OOB Score :  12.317726838530405\n"
     ]
    }
   ],
   "source": [
    "mse,oob_score,col_list=compute_mse()\n",
    "print('MSE : ',mse)\n",
    "print('OOB Score : ',oob_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26563853407383403, 0.2882674546532581, 0.2099446839148508, 0.28255944390734106, 0.2717120810926424, 0.220435242421669, 0.25230474366671335, 0.23302868310099656, 0.24736170048929837, 0.3092819989975252, 0.29601144692576836, 0.26059020959516327, 0.30591348969495125, 0.20206070491386227, 0.2746259886607548, 0.30497432799233454, 0.24507568078432096, 0.2488795173961554, 0.3455256290918091, 0.27716771320022593, 0.23822448179812775, 0.24845726502395618, 0.266492271791186, 0.3057965561507085, 0.23343083478687768, 0.2613958089358012, 0.21130641156578014, 0.2639158580948933, 0.26692833418642564, 0.2345155138041906, 0.2354080012210315, 0.33574548177413094, 0.1886631359867643, 0.25931911995047024, 0.3116431158788051]\n",
      "********************\n",
      "[14.430396226682955, 12.251391205851284, 12.970633519345663, 14.184682000746314, 14.701878853616208, 11.497559321918711, 13.127012952017989, 15.451443832711824, 14.101814199658211, 15.375245695082983, 13.554235699218731, 13.890673373988966, 14.307678592913586, 12.080637205966756, 13.944147161484873, 14.157033761456358, 13.353161208216735, 14.196468906496577, 13.975067380004358, 13.605997916844807, 12.418304029087487, 12.378707392978532, 12.478206899670054, 13.15931149024127, 12.185066837365126, 13.066694176868365, 12.58349833907892, 13.050616113059018, 16.149356097557853, 12.243708010924813, 12.860151446731162, 13.625373991695739, 13.362162815717095, 12.678636662342043, 13.67209218845048]\n"
     ]
    }
   ],
   "source": [
    "mse_ci=[] #MSE CI values\n",
    "oob_ci=[] #OOb score values\n",
    "for i in range(35):\n",
    "    mse,oob_score,col_list=compute_mse()\n",
    "    mse_ci.append(mse)\n",
    "    oob_ci.append(oob_score)\n",
    "print(mse_ci)\n",
    "print('*'*20)\n",
    "print(oob_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    " \n",
    "mean_mse=[] #1000 sample means of 30 MSE samples each\n",
    "mean_oob=[] #1000 sample means of 30 OOB Score samples each\n",
    "for i in range(1000):\n",
    "    s1=resample(mse_ci,n_samples=30)\n",
    "    m1=np.mean(s1)\n",
    "    mean_mse.append(m1)\n",
    "    \n",
    "    s2=resample(oob_ci,n_samples=30)\n",
    "    m2=np.mean(s2)\n",
    "    mean_oob.append(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% CI of mean of MSE : [0.2505252551420549 , 0.27584536147067124]\n",
      "95% CI of mean of OOB score : [13.101408021992242 , 13.825853127206175]\n"
     ]
    }
   ],
   "source": [
    "alpha=0.95\n",
    "p=((1-alpha)/2)*100\n",
    "l1=np.percentile(mean_mse,p)\n",
    "p=(alpha+((1-alpha)/2))*100\n",
    "u1=np.percentile(mean_mse,p)\n",
    "print('95% CI of mean of MSE : [{} , {}]'.format(l1,u1))\n",
    "\n",
    "p=((1-alpha)/2)*100\n",
    "l2=np.percentile(mean_oob,p)\n",
    "p=(alpha+((1-alpha)/2))*100\n",
    "u2=np.percentile(mean_oob,p)\n",
    "print('95% CI of mean of OOB score : [{} , {}]'.format(l2,u2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted price of the house :  20.811565189140325\n"
     ]
    }
   ],
   "source": [
    "#input query\n",
    "xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60]\n",
    "\n",
    "y_p=[]\n",
    "for i in range(30):\n",
    "    #each tree has input with the same column indecies with which it was trained\n",
    "    x1=np.array([xq[j] for j in col_list[i]]).reshape(1,-1)\n",
    "    y_p.append(regressor[i+1].predict(x1))\n",
    "print('predicted price of the house : ',np.mean(y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bootstrap_Random_Forest_instructions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
